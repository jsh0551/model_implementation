{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JSH\\Desktop\\MyGithub\\paper_implementation\\.paper_venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms,datasets\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw\n",
    "import random\n",
    "import torch.optim as optim\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JSH\\Desktop\\MyGithub\\paper_implementation\\.paper_venv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\JSH\\Desktop\\MyGithub\\paper_implementation\\.paper_venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load pretrained model.\n"
     ]
    }
   ],
   "source": [
    "from SSD import SSD\n",
    "num_classes = 10\n",
    "model = SSD()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from SSD import SSD_Dataset\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import transforms as A_transforms\n",
    "with open('../.data/train_ann.json','r') as f:\n",
    "    json_file = json.load(f)\n",
    "with open('../.data/val_ann.json','r') as f:\n",
    "    val_json_file = json.load(f)\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.Resize(300,300),\n",
    "    # A.RandomCrop(width=400, height=400),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    # A.RandomBrightnessContrast(p=0.2),\n",
    "    A.Rotate(limit=(30,40),p=1),\n",
    "    A.Normalize(mean=0.5, std=0.5),\n",
    "    # A_transforms.ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='coco',label_fields=['labels']))\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(300,300),\n",
    "    A.Normalize(mean=0.5, std=0.5),\n",
    "], bbox_params=A.BboxParams(format='coco' ,label_fields=['labels']))\n",
    "\n",
    "SSD_dataset = SSD_Dataset(json_file, train=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(SSD_dataset, batch_size=4,shuffle=False, collate_fn=SSD_dataset.collate_fn, num_workers=1)\n",
    "\n",
    "val_SSD_dataset = SSD_Dataset(val_json_file,train=False, transform=val_transform)\n",
    "valid_loader = torch.utils.data.DataLoader(val_SSD_dataset, batch_size=1,shuffle=False, collate_fn=SSD_dataset.collate_fn, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = random.randint(1,1000)\n",
    "image,boxes,label = SSD_dataset[n]\n",
    "to_pil = transforms.ToPILImage()\n",
    "img = to_pil(image).convert('RGB')\n",
    "draw = ImageDraw.Draw(img)\n",
    "fig , ax = plt.subplots(1,1,figsize = (12,12))\n",
    "for i,box in enumerate(boxes):\n",
    "    x1,y1,x2,y2 = box[0]-box[2]/2, box[1]-box[3]/2, box[0]+box[2]/2, box[1]+box[3]/2\n",
    "    draw.rectangle((x1,y1,x2,y2),outline=(0,0,0), width=3)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SSD import SSD_Loss\n",
    "from SSD import convert_to_ratio, object_detection, cal_mAP50\n",
    "\n",
    "learning_rate = 1e-4\n",
    "# momentum = 0.9\n",
    "weight_decay = 0.0005\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "criterion = SSD_Loss()\n",
    "\n",
    "epochs = 30\n",
    "all_train = len(train_loader)\n",
    "all_valid = len(valid_loader)\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for images, bboxes, labels in tqdm(train_loader):\n",
    "        images = images.to(device)\n",
    "        bboxes = [box.to(device) for box in bboxes]\n",
    "        labels = [label.to(device) for label in labels]\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred_bboxes,pred_labels = model(images)\n",
    "        \n",
    "        loss = criterion(pred_bboxes, pred_labels, bboxes, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss/all_train\n",
    "    print(f\"[epoch  {epoch+1}] : train loss : {train_loss}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    all_pred_bboxes, all_pred_labels, all_target_bboxes, all_target_labels = [], [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, target_bboxes, target_labels in tqdm(valid_loader):\n",
    "            images = images.to(device)\n",
    "            target_bboxes = [box.to(device) for box in target_bboxes]\n",
    "            target_labels = [label.to(device) for label in target_labels]\n",
    "            \n",
    "            pred_bboxes,pred_labels = model(images)\n",
    "            loss = criterion(pred_bboxes, pred_labels, target_bboxes, target_labels)\n",
    "            val_loss += loss/all_valid\n",
    "\n",
    "            target_ratio_bboxes = convert_to_ratio(images[0] ,target_bboxes)\n",
    "            all_pred_bboxes.append(pred_bboxes)\n",
    "            all_pred_labels.append(pred_labels)\n",
    "\n",
    "            all_target_bboxes.append(target_ratio_bboxes)\n",
    "            all_target_labels.append(target_labels)\n",
    "        print(f\"[epoch  {epoch+1}] : valid loss : {val_loss}\")\n",
    "\n",
    "        if  epoch%5 == 4:\n",
    "          all_img_boxes, all_img_labels, all_img_scores = object_detection(all_pred_bboxes, all_pred_labels)\n",
    "          mAP50_dict = cal_mAP50(all_img_boxes, all_img_labels, all_img_scores, all_target_bboxes, all_target_labels)\n",
    "          mAP50 = 0\n",
    "          print(f'====================================')\n",
    "          print(f'val_loss : {val_loss}')\n",
    "          for category in json_file['categories']:\n",
    "              c_id = category['id']\n",
    "              c_name = category['name']\n",
    "              print(f'{c_name}({c_id}) : {mAP50_dict[c_id]}')\n",
    "              mAP50 += mAP50_dict[c_id]/num_classes\n",
    "          print(f'------------------------------------')\n",
    "          print(f'    ** mAP50 : {mAP50} **')\n",
    "          print(f'====================================')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 ('.paper_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba55624f8f5dfc5da202e05117f540c3b5c190fa8630c7fe4435eb273edcce18"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
